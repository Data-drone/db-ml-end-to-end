{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build out Featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"environment\", \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_env = dbutils.widgets.get(\"environment\")\n",
    "curr_catalog = f'brian_ml_{curr_env}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import FloatType, IntegerType, StringType\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=IntegerType())\n",
    "def is_weekend(dt):\n",
    "    tz = \"America/New_York\"\n",
    "    return int(dt.astimezone(timezone(tz)).weekday() >= 5)  # 5 = Saturday, 6 = Sunday\n",
    "\n",
    "\n",
    "def filter_df_by_ts(df, ts_column, start_date, end_date):\n",
    "    if ts_column and start_date:\n",
    "        df = df.filter(col(ts_column) >= start_date)\n",
    "    if ts_column and end_date:\n",
    "        df = df.filter(col(ts_column) < end_date)\n",
    "    return df\n",
    "\n",
    "def pickup_features_fn(df, ts_column, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Computes the pickup_features feature group.\n",
    "    To restrict features to a time range, pass in ts_column, start_date, and/or end_date as kwargs.\n",
    "    \"\"\"\n",
    "    df = filter_df_by_ts(df, ts_column, start_date, end_date)\n",
    "    pickupzip_features = (\n",
    "        df.groupBy(\n",
    "            \"pickup_zip\", window(\"tpep_pickup_datetime\", \"1 hour\", \"15 minutes\")\n",
    "        )  # 1 hour window, sliding every 15 minutes\n",
    "        .agg(\n",
    "            mean(\"fare_amount\").alias(\"mean_fare_window_1h_pickup_zip\"),\n",
    "            count(\"*\").alias(\"count_trips_window_1h_pickup_zip\"),\n",
    "        )\n",
    "        .select(\n",
    "            col(\"pickup_zip\").alias(\"zip\"),\n",
    "            unix_timestamp(col(\"window.end\")).cast(\"timestamp\").alias(\"ts\"),\n",
    "            col(\"mean_fare_window_1h_pickup_zip\").cast(FloatType()),\n",
    "            col(\"count_trips_window_1h_pickup_zip\").cast(IntegerType()),\n",
    "        )\n",
    "    )\n",
    "    return pickupzip_features\n",
    "\n",
    "\n",
    "def dropoff_features_fn(df, ts_column, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Computes the dropoff_features feature group.\n",
    "    To restrict features to a time range, pass in ts_column, start_date, and/or end_date as kwargs.\n",
    "    \"\"\"\n",
    "    df = filter_df_by_ts(df, ts_column, start_date, end_date)\n",
    "    dropoffzip_features = (\n",
    "        df.groupBy(\"dropoff_zip\", window(\"tpep_dropoff_datetime\", \"30 minute\"))\n",
    "        .agg(count(\"*\").alias(\"count_trips_window_30m_dropoff_zip\"))\n",
    "        .select(\n",
    "            col(\"dropoff_zip\").alias(\"zip\"),\n",
    "            unix_timestamp(col(\"window.end\")).cast(\"timestamp\").alias(\"ts\"),\n",
    "            col(\"count_trips_window_30m_dropoff_zip\").cast(IntegerType()),\n",
    "            is_weekend(col(\"window.end\")).alias(\"dropoff_is_weekend\"),\n",
    "        )\n",
    "    )\n",
    "    return dropoffzip_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Inspect Data\n",
    "\n",
    "Lets load our raw data and see what it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = spark.table(f'{curr_catalog}.warehouse.raw_data')\n",
    "display(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "pickup_features = pickup_features_fn(\n",
    "    df=raw_data,\n",
    "    ts_column=\"tpep_pickup_datetime\",\n",
    "    start_date=datetime(2016, 1, 1),\n",
    "    end_date=datetime(2016, 1, 31),\n",
    ")\n",
    "dropoff_features = dropoff_features_fn(\n",
    "    df=raw_data,\n",
    "    ts_column=\"tpep_dropoff_datetime\",\n",
    "    start_date=datetime(2016, 1, 1),\n",
    "    end_date=datetime(2016, 1, 31),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pickup_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dropoff_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Feature Table Definitions\n",
    "\n",
    "We will use SQL Syntax to create the feature tables first before we write to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS brian_ml_${environment}.warehouse.trip_pickup_time_series_features(\n",
    "  zip INT NOT NULL,\n",
    "  ts TIMESTAMP NOT NULL,\n",
    "  mean_fare_window_1h_pickup_zip FLOAT,\n",
    "  count_trips_window_1h_pickup_zip INT,\n",
    "  CONSTRAINT trip_pickup_time_series_features_pk PRIMARY KEY (zip, ts TIMESERIES)\n",
    ")\n",
    "COMMENT \"Taxi Fares. Pickup Time Series Features\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS brian_ml_${environment}.warehouse.trip_dropoff_time_series_features(\n",
    "  zip INT NOT NULL,\n",
    "  ts TIMESTAMP NOT NULL,\n",
    "  count_trips_window_30m_dropoff_zip INT,\n",
    "  dropoff_is_weekend INT,\n",
    "  CONSTRAINT trip_dropoff_time_series_features_pk PRIMARY KEY (zip, ts TIMESERIES)\n",
    ")\n",
    "COMMENT \"Taxi Fares. Dropoff Time Series Features\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Feature Tables\n",
    "\n",
    "We will create a feature table and write to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks import feature_store\n",
    "\n",
    "fs = feature_store.FeatureStoreClient()\n",
    "\n",
    "# To append we can set the `mode` to be overwrite`\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "fs.write_table(\n",
    "    name=f\"{curr_catalog}.warehouse.trip_pickup_time_series_features\",\n",
    "    df=pickup_features,\n",
    "    mode='overwrite'\n",
    ")\n",
    "fs.write_table(\n",
    "    name=f\"{curr_catalog}.warehouse.trip_dropoff_time_series_features\",\n",
    "    df=dropoff_features,\n",
    "    mode='overwrite'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
